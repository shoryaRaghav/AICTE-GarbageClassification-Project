{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca4f5a85-2d02-4ca0-8ae4-36f3ee473c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1343 images belonging to 6 classes.\n",
      "Found 332 images belonging to 6 classes.\n",
      "Class indices: {'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}\n"
     ]
    }
   ],
   "source": [
    "# Load the Dataset (Only Loading for Now)\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set path to your dataset\n",
    "dataset_path = \"TrashType_Image_Dataset\"  # Replace with actual path\n",
    "\n",
    "# Set image parameters\n",
    "img_height, img_width = 128, 128\n",
    "batch_size = 32\n",
    "\n",
    "# Create basic data generator (no augmentation or rescaling yet)\n",
    "datagen = ImageDataGenerator(\n",
    "    validation_split=0.2  # 80% train, 20% validation\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "val_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Print class labels\n",
    "print(\"Class indices:\", train_data.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff346c0a-df39-4e43-a41c-387fe5dd04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done. Total corrupted images removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle Missing or Corrupted Images\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "dataset_path = \"TrashType_Image_Dataset\"  # Replace with actual path\n",
    "corrupt_count = 0\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            # Try to open image\n",
    "            img = Image.open(file_path)\n",
    "            img.verify()  # Verify image integrity\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(f\"Corrupted image found and removed: {file_path}\")\n",
    "            os.remove(file_path)\n",
    "            corrupt_count += 1\n",
    "\n",
    "print(f\"‚úÖ Done. Total corrupted images removed: {corrupt_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad9fb4f6-e6b6-4049-a35e-6e076a3fdc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ No empty folders found.\n"
     ]
    }
   ],
   "source": [
    "# Check for Empty Folders\n",
    "\n",
    "empty_folders = []\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    if os.path.isdir(folder_path) and len(os.listdir(folder_path)) == 0:\n",
    "        empty_folders.append(folder_path)\n",
    "\n",
    "if empty_folders:\n",
    "    print(\"‚ö†Ô∏è Empty class folders found:\")\n",
    "    for f in empty_folders:\n",
    "        print(f)\n",
    "else:\n",
    "    print(\"‚úÖ No empty folders found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97c13b28-805d-4996-bb00-b9551a5d5045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Duplicate removal complete. Total duplicates removed: 0\n"
     ]
    }
   ],
   "source": [
    "# Remove Duplicate Images from Dataset\n",
    "import os\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "\n",
    "seen_hashes = set()\n",
    "duplicates_removed = 0\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                hash_val = imagehash.average_hash(img)\n",
    "\n",
    "            if hash_val in seen_hashes:\n",
    "                os.remove(file_path)\n",
    "                print(f\"üóëÔ∏è Removed duplicate: {file_path}\")\n",
    "                duplicates_removed += 1\n",
    "            else:\n",
    "                seen_hashes.add(hash_val)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Duplicate removal complete. Total duplicates removed: {duplicates_removed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6a39344-ad6e-4a7b-860e-238ca26d45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images to a uniform size (e.g., 128x128 or 224x224)\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Choose target size\n",
    "target_size = (128, 128)  # or (224, 224)\n",
    "\n",
    "resized_count = 0\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                img = img.convert(\"RGB\")  # Ensure 3 channels\n",
    "                img_resized = img.resize(target_size, Image.ANTIALIAS)\n",
    "                img_resized.save(file_path)\n",
    "                resized_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5d6e0dd-4233-4856-af60-f17e18443df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1343 images belonging to 6 classes.\n",
      "Found 332 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "dataset_path = \"TrashType_Image_Dataset\"\n",
    "# Define image size and batch\n",
    "img_height, img_width = 128, 128\n",
    "batch_size = 32\n",
    "\n",
    "# Create generator with normalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,         # Normalize to [0, 1]\n",
    "    validation_split=0.2    # Still keeping 20% for validation\n",
    ")\n",
    "\n",
    "# Training data\n",
    "train_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation data\n",
    "val_data = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68813c3f-e591-434b-9db6-63112e1aec6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
